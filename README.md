# History of Massachusetts Q&A Bot

## Live Application Links
[![Live App](https://img.shields.io/badge/LIVE%20APP-green?style=for-the-badge)](https://lfkzplbnxa55eeb2jwdvka.streamlit.app/)
[![Demo Link](https://img.shields.io/badge/DEMO%20LINK-red?style=for-the-badge)](https://drive.google.com/file/d/1iDpnKziA_2BT4bhlxEqdznhVWQsYglYq/view?usp=sharing)

## Tools & Technologies
[![Python](https://img.shields.io/badge/Python-lightblue?style=for-the-badge&logo=python)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-lightcoral?style=for-the-badge&logo=streamlit)](https://streamlit.io/)
[![OpenAI](https://img.shields.io/badge/OpenAI-black?style=for-the-badge&logo=openai)](https://openai.com/)
[![FAISS](https://img.shields.io/badge/FAISS-green?style=for-the-badge)](https://ai.meta.com/tools/faiss/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-orange?style=for-the-badge&logo=huggingface)](https://huggingface.co/)
[![Pandas](https://img.shields.io/badge/Pandas-darkblue?style=for-the-badge&logo=pandas)](https://pandas.pydata.org/)
[![Wikipedia API](https://img.shields.io/badge/Wikipedia%20API-lightgrey?style=for-the-badge)](https://pypi.org/project/Wikipedia-API/)
[![LangChain](https://img.shields.io/badge/LangChain-purple?style=for-the-badge)](https://www.langchain.com/)






## Overview

The **History of Massachusetts Q&A Bot** is a Retrieval-Augmented Generation (RAG) system designed to answer questions about Massachusetts history. It leverages Wikipedia as the primary knowledge source, employs vector embeddings for semantic search, and integrates OpenAI's GPT-3.5-turbo for natural language responses. The system processes historical data into Markdown and CSV formats and uses a Streamlit interface for user interaction.

## Features

- **Data Collection**: Extracts structured data from Wikipedia using the `wikipedia-api` library.
- **Semantic Search**: Utilizes FAISS for fast and accurate similarity-based document retrieval.
- **Concise Answers**: GPT-3.5-turbo generates precise responses to user queries.
- **Interactive Frontend**: Streamlit provides an accessible interface for user interaction.

## File Structure

- **`Wiki_Scraper.py`**: Script for scraping and processing Wikipedia pages. Saves data as CSV and Markdown files. 
- **`RAGChat.py`**: Core implementation of the RAG system, including document chunking, embedding generation, and retrieval logic. 
- **`app.py`**: Streamlit application for hosting the Q&A bot. Handles user queries and displays responses interactively. 
- **`requirements.txt`**: Lists Python dependencies required for the project. 
- **`output.md`**: Example of Markdown output generated by the Wikipedia scraper.
- **`markdown.py`**: Converts CSV files into Markdown format, organizing content by headings and sections for improved readability.

![image](https://github.com/user-attachments/assets/8bf184c5-bfa8-4b24-b4d3-b0b5229a9577)


## Installation

### Prerequisites
- Python 3.8 or higher
- OpenAI API key

### Setup Steps
1. **Clone the Repository**:
   ```bash
   git clone <repository_url>
   cd <repository_folder>
   ```

2. **Install Dependencies**:
   Use `pip` to install the required libraries:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure OpenAI API**:
   Set your OpenAI API key in the Streamlit secrets file:
   ```bash
   mkdir -p ~/.streamlit
   echo "[secrets]\napi='<YOUR_API_KEY>'" > ~/.streamlit/secrets.toml
   ```
4. **Run the Wiki Scraper Script**:
   Fetch, process, and save Wikipedia content as CSV files
   ```bash
   python Wiki_Scraper.py
   ```
5. **Generate Markdown Files**:
   Convert the CSV files into a single Markdown file for easy sharing or documentation
   ```bash
   python markdown.py
   ```
6. **Run Streamlit**:
   Host the application on your local network
   ```bash
   streamlit run app.py
   ```
## Project Tree
```
├── .streamlit/
│   ├── secrets.toml
├── Data/
│   ├── raw_data/
│   │   ├── History_of_Massachusetts.csv
│   │   ├── Massachusetts.csv
│   │   ├── Massachusetts_Bay_Colony.csv
│   │   ├── ... (other Wikipedia-sourced CSV files)
│   ├── combined_data.csv
│   ├── output.md
├── Report/
│   ├── Project_Report.pdf
│   ├── Application.png
├── RAGChat.py
├── Wiki_Scraper.py
├── app.py
├── logo.png
├── markdown.py
├── requirements.txt
├── README.md
```

## Usage

### Data Preparation

This script prepares and processes data from multiple Wikipedia pages into structured formats for further analysis and documentation. The following steps outline the process:

### 1. Setup and Initialization
- **Directory Creation**: A directory named `raw_data` is created if it does not already exist. This directory will store individual CSV files generated for each Wikipedia page.
- **Wikipedia API Initialization**: The `wikipediaapi` library is initialized for the English Wikipedia with a custom user-agent (`RAG 2 Riches`) to ensure smooth interaction with the API.

### 2. Fetching and Processing Wikipedia Data
- **Page Retrieval**: The script uses the `wikipediaapi` library to fetch data for a list of specified Wikipedia pages.
- **Exclusion of Unwanted Sections**: Sections such as `See also`, `References`, `Bibliography`, `External links`, `Explanatory notes`, and `Further reading` are excluded to focus on relevant content.
- **Recursive Section Parsing**: A recursive function processes each section and its subsections to extract meaningful text. Full section titles are generated to maintain context.
- **Saving as CSV**: The extracted data for each page is saved as a CSV file in the `raw_data` folder. Each CSV file includes two columns:
  - `section`: The section title.
  - `text`: The content.

### 3. Combining Data
- **Loading Individual CSV Files**: All CSV files in the `raw_data` folder are loaded into separate pandas DataFrames.
- **Combining DataFrames**: These DataFrames are concatenated into a single DataFrame containing all the sections and text from the processed Wikipedia pages.
- **Saving Combined Data**: The combined DataFrame is saved as a single CSV file named `combined_data.csv`.

### 4. Markdown Conversion
- **CSV to Markdown Table**: The combined CSV file (`combined_data.csv`) is converted into a Markdown table using the `pandas` library. The Markdown table is saved as `combined_data.md`.
- **CSV to Detailed Markdown Document**:
  - Each CSV file is individually processed to create a detailed Markdown file.
  - The Markdown file includes:
    - A main heading for each Wikipedia page (based on the CSV file name).
    - Subheadings for each section title.
    - Corresponding text content under each section.
  - The final Markdown document (`output.md`) is saved in the `raw_data` folder.

### 5. Error Handling
- The script checks if each Wikipedia page exists. If not, an error message is displayed, and the page is skipped.
- During Markdown conversion, the script ensures that required columns (`section` and `text`) exist in each CSV file, displaying warnings for missing columns.

This structured process ensures that the Wikipedia data is well-organized, easy to analyze, and available in both CSV and Markdown formats for different use cases.

### Running the Application
1. **Start the Streamlit Interface**:
   Launch the Q&A bot using:
   ```bash
   streamlit run app.py
   ```

2. **Interact with the Bot**:
   Enter historical queries and receive concise, contextually accurate answers.

### Example Query
**Input**: "What is the history of the Massachusetts Bay Colony?"  
**Output**: Founded in 1630, became Province of Massachusetts Bay in 1691–92

<img width="1710" alt="Application" src="https://github.com/user-attachments/assets/b0fe808c-96f0-4097-9209-b8892b243fb0" />


## Key Components

### Data Scraping
- Extracts content from Wikipedia.
- Converts processed content into CSV and Markdown formats for downstream use.

### Semantic Search and RAG Pipeline
- **Embeddings**: HuggingFace's `all-MiniLM-L6-v2` for semantic encoding.
- **Retriever**: FAISS for high-speed similarity search across document chunks.
- **Compressor**: LLMChainExtractor to condense retrieved content before passing it to the language model.

### Language Model
- **Model**: GPT-3.5-turbo via OpenAI API.
- **Function**: Generates natural language responses based on compressed, contextually relevant data.

## Dependencies
Key libraries and tools:
- `langchain`, `faiss-cpu`, `wikipedia-api`, `streamlit`, `openai`, `transformers`, `sentence-transformers`.

Refer to `requirements.txt` for the full list of dependencies.


## Troubleshooting
- **API Issues**: Ensure the OpenAI API key is valid and set up correctly in Streamlit secrets.
- **Vector Store**: Verify FAISS index is correctly initialized with the processed data.
- **UI Errors**: Restart the Streamlit server and check the console logs for detailed error messages.
